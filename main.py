# Does Interference Exist When Training a Once-for-All Network?
# Jordan Shipard, Arnold Wiliem, Clinton Fookes
# Computer Vision and Pattern Recognition Embedded Vision Workshop (CVPRW), 2022.
#
# Implementation based on:
# Once for All: Train One Network and Specialize it for Efficient Deployment
# Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, Song Han
# International Conference on Learning Representations (ICLR), 2020.

import argparse

from ofa.imagenet_classification.run_manager.run_config import CIFAR100RunConfig, CIFAR10RunConfig, FMNISTRunConfig, MNISTRunConfig, ImagenetRunConfig
import numpy as np  
import os
import random
import time

import torch

from ofa.imagenet_classification.elastic_nn.modules.dynamic_op import DynamicSeparableConv2d
from ofa.imagenet_classification.elastic_nn.networks import OFAMobileNetV3, OFAMobileNetV3Grey, OFAProxylessNASNets, OFAResNets
from ofa.imagenet_classification.networks import MobileNetV3Large

from ofa.imagenet_classification.run_manager.run_manager import RunManager
from torchvision.models import mobilenet_v3_large
from ofa.utils import MyRandomResizedCrop


parser = argparse.ArgumentParser()
args = parser.parse_args()
args.path = 'Trained_Networks/RSS-Net'
args.task = 'RSS'
args.phase = 2
base_epochs = 180
base_learning_rate = 0.01
args.manual_seed = 0
args.kd_ratio = 0
args.kd_type = 'ce'

if args.task == 'kernel':
    args.dynamic_batch_size = 1
    args.n_epochs = base_epochs + round(base_epochs/1.5)
    args.base_lr = base_learning_rate/2.7
    args.warmup_epochs = 0
    args.warmup_lr = -1
    args.ks_list = '3,5,7'
    args.expand_list = '6'
    args.depth_list = '4'
    args.kd_ratio = 1

elif args.task == 'depth':
    args.dynamic_batch_size = 2
    if args.phase == 1:
        args.n_epochs = base_epochs + round(base_epochs/1.5) + round(base_epochs/7.2)
        args.base_lr = base_learning_rate/32.5
        args.warmup_epochs = 0
        args.warmup_lr = -1
        args.ks_list = '3,5,7'
        args.expand_list = '6'
        args.depth_list = '3,4'
        args.kd_ratio = 1
    else:
        args.n_epochs = base_epochs + round(base_epochs/1.5) + round(base_epochs/7.2) + round(base_epochs/1.5)
        args.base_lr = base_learning_rate/10.8
        args.warmup_epochs = 0
        args.warmup_lr = -1
        args.ks_list = '3,5,7'
        args.expand_list = '6'
        args.depth_list = '2,3,4'
        args.kd_ratio = 1
elif args.task == 'expand':
    args.dynamic_batch_size = 4
    if args.phase == 1:
        args.n_epochs = base_epochs + round(base_epochs/1.5) + round(base_epochs/7.2) + round(base_epochs/1.5) + round(base_epochs/7.2)
        args.base_lr = base_learning_rate/32.5
        args.warmup_epochs = 0
        args.warmup_lr = -1
        args.ks_list = '3,5,7'
        args.expand_list = '4,6'
        args.depth_list = '2,3,4'
        args.kd_ratio = 1
    else:
        args.n_epochs = base_epochs + round(base_epochs/1.5) + round(base_epochs/7.2) + round(base_epochs/1.5) + round(base_epochs/7.2) + round(base_epochs/1.5)
        args.base_lr = base_learning_rate/10.8
        args.warmup_epochs = 0
        args.warmup_lr = -1
        args.ks_list = '3,5,7'
        args.expand_list = '3,4,6'
        args.depth_list = '2,3,4'
        args.kd_ratio = 1
else:
    args.n_epochs = base_epochs 
    args.base_lr = base_learning_rate
    args.warmup_epochs = 5
    args.warmup_lr = 0.01
    args.ks_list = '3,5,7'
    args.expand_list = '3,4,6'
    args.depth_list = '2,3,4'


# More settings relating to training
args.lr_schedule_type = 'cosine'
# args.lr_schedule_type = None

args.base_batch_size = 64
args.valid_size = 10000

args.opt_type = 'sgd'
args.momentum = 0.9
args.no_nesterov = False
args.weight_decay = 3e-5
args.label_smoothing = 0
args.no_decay_keys = 'bn#bias'
args.fp16_allreduce = False

args.model_init = 'he_fout'
args.validation_frequency = 10
args.print_frequency = 10

args.n_worker = 2
args.resize_scale = 0.08
args.distort_color = 'tf'
args.image_size = '32'
args.continuous_size = True
args.not_sync_distributed_image_size = False

args.bn_momentum = 0.1
args.bn_eps = 1e-5
args.dropout = 0.1
args.base_stage_width = 'proxyless'

args.width_mult_list = '1.0'
args.dy_conv_scaling_mode = 1
args.independent_distributed_sampling = False



if __name__ == '__main__':
    os.makedirs(args.path, exist_ok=True)

    torch.cuda.set_device("cuda:0")

    num_gpus = 1
    seed = time.time()

    torch.manual_seed(args.manual_seed)
    torch.cuda.manual_seed_all(args.manual_seed)
    np.random.seed(int(seed))
    random.seed(int(seed))

    args.image_size = [int(img_size) for img_size in args.image_size.split(',')]    # Converts the list of image sizes to an array of ints
    if len(args.image_size) == 1:               # if there is only one size 
        args.image_size = args.image_size[0]    ## Change it from a array of 1 element to just the int
    MyRandomResizedCrop.CONTINUOUS = args.continuous_size                               # Just sets .CONTINUOUS to True
    MyRandomResizedCrop.SYNC_DISTRIBUTED = not args.not_sync_distributed_image_size     # Sets to not the argument ~= True

    # build run config from args
    args.lr_schedule_param = None   # first definition
    args.opt_param = {              # same
        'momentum': args.momentum,  # = 0.1
        'nesterov': not args.no_nesterov,   # = true
    }

    args.init_lr = args.base_lr * num_gpus  # linearly rescale the learning rate
    if args.warmup_lr < 0:              # If no warmup [CHECK WHAT WARMUP IS]
        args.warmup_lr = args.base_lr   ## set it to the base lr
    args.train_batch_size = args.base_batch_size        # = 64
    args.test_batch_size = args.base_batch_size * 4     # = 64 * 4 = 256
    run_config = CIFAR100RunConfig(**args.__dict__)

    if args.dy_conv_scaling_mode == -1:     # == 1 by deault
        args.dy_conv_scaling_mode = None
    DynamicSeparableConv2d.KERNEL_TRANSFORM_MODE = args.dy_conv_scaling_mode    # Not sure what it does yet but it gets set to 1  

    args.width_mult_list = [float(width_mult) for width_mult in args.width_mult_list.split(',')]
    args.ks_list = [int(ks) for ks in args.ks_list.split(',')]
    args.expand_list = [int(e) for e in args.expand_list.split(',')]
    args.depth_list = [int(d) for d in args.depth_list.split(',')]

    args.width_mult_list = args.width_mult_list[0] if len(args.width_mult_list) == 1 else args.width_mult_list

    # Creates the OFA Mobilenet network from the params 
    net = OFAMobileNetV3(
        n_classes=run_config.data_provider.n_classes, bn_param=(args.bn_momentum, args.bn_eps),
        dropout_rate=args.dropout, base_stage_width=args.base_stage_width, width_mult=args.width_mult_list,
        ks_list=args.ks_list, expand_ratio_list=args.expand_list, depth_list=args.depth_list
    )

    if args.kd_ratio > 0:
        args.teacher_model = MobileNetV3Large(
            n_classes=run_config.data_provider.n_classes, bn_param=(args.bn_momentum, args.bn_eps),
            dropout_rate=0, width_mult=1.0, ks=7, expand_ratio=6, depth_param=4,
        )
        args.teacher_model.cuda()
    else:
        args.teacher_model = None

    run_manager = RunManager(args.path, net, run_config)
    run_manager.load_model()

    train_start = time.time()
    if args.task == "super":
        run_manager.train(args, "supernet")

    elif args.task == "RSS":
        run_manager.train(args, "RSS")

    elif args.task == "RSS Anchor":
        run_manager.train(args, "RSS Anchor")

    elif args.task == "anchor":
        anchor = {"ks":[7]*20, "e":[6]*20, "d":[4]*5}
        run_manager.network.set_active_subnet(anchor["ks"], anchor['e'], anchor['d'])
        run_manager.train(args, "supernet")

    elif args.task == "eval subnets":
        run_manager.eval_subnet_population(True)

    elif args.task == "net flops":
        subnet = {"ks":[7]*20, "e":[4]*20, "d":[3]*5}
        run_manager.get_net_flops(subnet)

    elif args.task == "flops bucket":
        buckets = [4, 6, 8, 10, 12]
        n = 100
        run_manager.eval_subnet_population(buckets, n)
        
    else:
        from ofa.imagenet_classification.elastic_nn.training.progressive_shrinking import validate, train, train_elastic_depth, train_elastic_expand

        args.resume = True
        validate_func_dict = {'image_size_list':[args.image_size],
                          'ks_list': sorted({min(args.ks_list), max(args.ks_list)}),
                          'expand_ratio_list': sorted({min(args.expand_list), max(args.expand_list)}),
                          'depth_list': sorted({min(net.depth_list), max(net.depth_list)})}

        if args.task == "kernel":
            validate_func_dict['ks_list'] = sorted(args.ks_list)
            train(run_manager, args,
            lambda _run_manager, epoch, is_test: validate(_run_manager, epoch, is_test, **validate_func_dict))

            run_manager.save_model({
                'epoch': args.n_epochs,
                'best_acc': run_manager.best_acc,
                'training_acc': run_manager.train_acc,
                'train_times': run_manager.train_times,
                'optimizer': run_manager.optimizer.state_dict(),
                'state_dict': run_manager.network.state_dict(),
                'subnets_trained': run_manager.subnets_trained
            }, model_name="KernelTrained.pth.tar" )

        elif args.task == "depth":
            train_elastic_depth(train, run_manager, args, validate_func_dict)
            run_manager.save_model({
                'epoch': args.n_epochs,
                'best_acc': run_manager.best_acc,
                'training_acc': run_manager.train_acc,
                'train_times': run_manager.train_times,
                'optimizer': run_manager.optimizer.state_dict(),
                'state_dict': run_manager.network.state_dict(),
                'subnets_trained': run_manager.subnets_trained
            }, model_name="DepthTrained.pth.tar" )
          
        elif args.task == "expand":
            train_elastic_expand(train, run_manager, args, validate_func_dict)
            run_manager.save_model({
                'epoch': args.n_epochs,
                'best_acc': run_manager.best_acc,
                'training_acc': run_manager.train_acc,
                'train_times': run_manager.train_times,
                'optimizer': run_manager.optimizer.state_dict(),
                'state_dict': run_manager.network.state_dict(),
                'subnets_trained': run_manager.subnets_trained
            }, model_name="ExpandTrained.pth.tar" )

        elif args.task == "RSS batch sampling":
            validate_func_dict = {'image_size_list': [args.image_size],
                          'ks_list': [max(args.ks_list)],
                          'expand_ratio_list': [max(args.expand_list)],
                          'depth_list': [max(net.depth_list)]}
            train(run_manager, args,
            lambda _run_manager, epoch, is_test: validate(_run_manager, epoch, is_test, **validate_func_dict))
            
    train_stop = time.time()
    print("Time taken: {}".format(train_stop-train_start))

